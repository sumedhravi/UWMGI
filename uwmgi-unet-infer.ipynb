{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!mkdir -p /tmp/pip/cache/\n!cp -r ../input/pytorch-segmentation-models-lib/efficientnet_pytorch-0.6.3 /tmp/pip/cache/efficientnet_pytorch-0.6.3\n!cp -r ../input/pytorch-segmentation-models-lib/pretrainedmodels-0.7.4 /tmp/pip/cache/pretrainedmodels-0.7.4\n!cp ../input/pytorch-segmentation-models-lib/segmentation_models_pytorch-0.2.0-py3-none-any.whl /tmp/pip/cache/\n!cp ../input/pytorch-segmentation-models-lib/timm-0.4.12-py3-none-any.whl /tmp/pip/cache/\n# !cp ../input/segmentationmodelspytorch/segmentation_models/timm-0.2.1-py3-none-any.whl /tmp/pip/cache/\n# !pip install --no-index --find-links /tmp/pip/cache/ efficientnet-pytorch\n# !pip install --no-index --find-links /tmp/pip/cache/ segmentation-models-pytorch","metadata":{"execution":{"iopub.status.busy":"2022-05-15T14:44:42.136795Z","iopub.execute_input":"2022-05-15T14:44:42.137744Z","iopub.status.idle":"2022-05-15T14:44:45.66459Z","shell.execute_reply.started":"2022-05-15T14:44:42.137702Z","shell.execute_reply":"2022-05-15T14:44:45.6636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -q /tmp/pip/cache/pretrainedmodels-0.7.4/pretrainedmodels-0.7.4\n!pip install -q /tmp/pip/cache/efficientnet_pytorch-0.6.3/efficientnet_pytorch-0.6.3\n!pip install -q /tmp/pip/cache/timm-0.4.12-py3-none-any.whl\n!pip install -q /tmp/pip/cache/segmentation_models_pytorch-0.2.0-py3-none-any.whl","metadata":{"execution":{"iopub.status.busy":"2022-05-15T14:44:45.666659Z","iopub.execute_input":"2022-05-15T14:44:45.666938Z","iopub.status.idle":"2022-05-15T14:46:45.765397Z","shell.execute_reply.started":"2022-05-15T14:44:45.666902Z","shell.execute_reply":"2022-05-15T14:46:45.76457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport torch, torchvision\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.models as models\nimport timm\nimport segmentation_models_pytorch as smp\nimport gc\n\n# Albumentations for augmentations\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nfrom PIL import Image\nimport cv2\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle\n%matplotlib inline\n\n# Sklearn\n# from sklearn.model_selection import StratifiedKFold, KFold, StratifiedGroupKFold\n\nimport os\nimport time\nimport random\nfrom glob import glob\nimport copy\nfrom pathlib import Path\nfrom typing import Callable\nfrom typing import List\nfrom typing import Optional\nfrom typing import Tuple\nfrom tqdm import tqdm\ntqdm.pandas()\n\n# For colored terminal text\nfrom colorama import Fore, Back, Style","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-15T15:26:22.66531Z","iopub.execute_input":"2022-05-15T15:26:22.66593Z","iopub.status.idle":"2022-05-15T15:26:22.683302Z","shell.execute_reply.started":"2022-05-15T15:26:22.665891Z","shell.execute_reply":"2022-05-15T15:26:22.682573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    IMG_SIZE = [320, 384]\n    KAGGLE_DIR = Path(\"/\") / \"kaggle\"\n    INPUT_DIR = KAGGLE_DIR / \"input\"\n    OUTPUT_DIR = KAGGLE_DIR / \"working\"\n\n    DATA_DIR = INPUT_DIR / \"uw-madison-gi-tract-image-segmentation\"\n#     INPUT_TRAIN_DIR = INPUT_DIR / \"uwmgisegmentationpreprocessed\"\n    DEBUG = False # Debug complete pipeline\n    N_SPLITS = 5\n    SEED = 42\n    USE_DEPTH = True # True for 2.5D data\n    USE_AUGS = True\n    BATCH_SIZE = 64\n    NUM_WORKERS = 2\n    ARCH = \"Unet\"\n    ENCODER_NAME = \"efficientnet-b1\"\n    ENCODER_WEIGHTS = \"imagenet\"\n    LOSS = [\"bce\", \"tversky\"]\n    NUM_CLASSES = 3\n    TRAIN_BS      = 64\n    VALID_BS      = TRAIN_BS*2\n    LEARNING_RATE = 2e-3\n    WEIGHT_DECAY = 1e-6\n    NUM_EPOCHS = 2 if DEBUG else 15\n    THR = 0.5\n    load_saved_weights = True\n    saved_weights_dir = INPUT_DIR / \"uwmgi-unet\"\n\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    FOLDS=[0,1] if DEBUG else [0]\n    \n    imp_color  = Fore.GREEN\n    reset_style = Style.RESET_ALL","metadata":{"execution":{"iopub.status.busy":"2022-05-15T15:43:01.066284Z","iopub.execute_input":"2022-05-15T15:43:01.066823Z","iopub.status.idle":"2022-05-15T15:43:01.074903Z","shell.execute_reply.started":"2022-05-15T15:43:01.066782Z","shell.execute_reply":"2022-05-15T15:43:01.074169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed = 42):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # When running on the CuDNN backend, two further options must be set\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    print('> SEEDING DONE')\n    \nset_seed(CFG.SEED)","metadata":{"execution":{"iopub.status.busy":"2022-05-15T14:47:15.197746Z","iopub.execute_input":"2022-05-15T14:47:15.198486Z","iopub.status.idle":"2022-05-15T14:47:15.208385Z","shell.execute_reply.started":"2022-05-15T14:47:15.19842Z","shell.execute_reply":"2022-05-15T14:47:15.207434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_metadata(row):\n    data = row['id'].split('_')\n    case = int(data[0].replace('case',''))\n    day = int(data[1].replace('day',''))\n    slice_ = int(data[-1])\n    row['case'] = case\n    row['day'] = day\n    row['slice'] = slice_\n    return row\n\ndef path2info(row):\n    path = row['image_path']\n    data = path.split('/')\n    slice_ = int(data[-1].split('_')[1])\n    case = int(data[-3].split('_')[0].replace('case',''))\n    day = int(data[-3].split('_')[1].replace('day',''))\n    width = int(data[-1].split('_')[2])\n    height = int(data[-1].split('_')[3])\n    row['height'] = height\n    row['width'] = width\n    row['case'] = case\n    row['day'] = day\n    row['slice'] = slice_\n#     row['id'] = f'case{case}_day{day}_slice_{slice_}'\n    return row","metadata":{"execution":{"iopub.status.busy":"2022-05-15T14:47:22.721958Z","iopub.execute_input":"2022-05-15T14:47:22.722546Z","iopub.status.idle":"2022-05-15T14:47:22.733285Z","shell.execute_reply.started":"2022-05-15T14:47:22.722509Z","shell.execute_reply":"2022-05-15T14:47:22.731706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ref: https://www.kaggle.com/paulorzp/run-length-encode-and-decode\ndef rle_decode(mask_rle, shape):\n    '''\n    mask_rle: run-length as string formated (start length)\n    shape: (height,width) of array to return \n    Returns numpy array, 1 - mask, 0 - background\n\n    '''\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    return img.reshape(shape)  # Needed to align to RLE direction\n\n\n# ref.: https://www.kaggle.com/stainsby/fast-tested-rle\ndef rle_encode(img):\n    '''\n    img: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    '''\n    pixels = img.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)","metadata":{"execution":{"iopub.status.busy":"2022-05-15T14:47:23.577473Z","iopub.execute_input":"2022-05-15T14:47:23.578068Z","iopub.status.idle":"2022-05-15T14:47:23.58601Z","shell.execute_reply.started":"2022-05-15T14:47:23.578034Z","shell.execute_reply":"2022-05-15T14:47:23.585341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_img(img, mask=None):\n    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n    img = clahe.apply(img)\n    plt.imshow(img, cmap='bone')\n    \n    if mask is not None:\n        # plt.imshow(np.ma.masked_where(mask!=1, mask), alpha=0.5, cmap='autumn')\n        plt.imshow(mask, alpha=0.5)\n        handles = [Rectangle((0,0),1,1, color=_c) for _c in [(0.667,0.0,0.0), (0.0,0.667,0.0), (0.0,0.0,0.667)]]\n        labels = [\"Large Bowel\", \"Small Bowel\", \"Stomach\"]\n        plt.legend(handles,labels)\n    plt.axis('off')","metadata":{"execution":{"iopub.status.busy":"2022-05-15T14:47:24.34113Z","iopub.execute_input":"2022-05-15T14:47:24.341712Z","iopub.status.idle":"2022-05-15T14:47:24.348992Z","shell.execute_reply.started":"2022-05-15T14:47:24.341673Z","shell.execute_reply":"2022-05-15T14:47:24.347973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_transforms(split='train', img_size = CFG.IMG_SIZE):\n    if split == 'valid':\n        return A.Compose([\n    #         A.Resize(img_size, interpolation=cv2.INTER_NEAREST),\n            ToTensorV2(transpose_mask=True)\n            ], p=1.0)\n    elif split == 'train':\n        return A.Compose([\n            A.HorizontalFlip(p=0.5),\n    #         A.VerticalFlip(p=0.5),\n            A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.05, rotate_limit=10, p=0.5),\n#             A.RandomResizedCrop(*img_size, scale=(0.3, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=1, always_apply=False, p=0.5),\n            A.OneOf([\n                A.GridDistortion(num_steps=5, distort_limit=0.05, p=1.0),\n    # #             A.OpticalDistortion(distort_limit=0.05, shift_limit=0.05, p=1.0),\n                A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=1.0)\n            ], p=0.25),\n            A.CoarseDropout(max_holes=8, max_height=img_size[0]//20, max_width=img_size[1]//20,\n                             min_holes=5, fill_value=0, mask_fill_value=0, p=0.5),\n            ToTensorV2(transpose_mask=True)\n            ], p=1.0)","metadata":{"execution":{"iopub.status.busy":"2022-05-15T14:47:25.716093Z","iopub.execute_input":"2022-05-15T14:47:25.716352Z","iopub.status.idle":"2022-05-15T14:47:25.724644Z","shell.execute_reply.started":"2022-05-15T14:47:25.716323Z","shell.execute_reply":"2022-05-15T14:47:25.723803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Dataset(torch.utils.data.Dataset):\n    def __init__(self, df, label=False, transforms=None):\n        self.df         = df\n        self.label      = label\n        self.img_paths  = df['image_paths'].tolist()\n        self.ids        = df['id'].tolist()\n        if 'mask_path' in df.columns:\n            self.mask_paths  = df['mask_path'].tolist()\n        else:\n            self.mask_paths = None\n        self.transforms = transforms\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        img_path  = self.img_paths[index]\n        id_       = self.ids[index]\n        img = []\n        img, shape0 = self.load_imgs(img_path)\n        h, w = shape0\n        if self.label:\n            mask_path = self.mask_paths[index]\n            mask = load_mask(mask_path)\n            if self.transforms:\n                data = self.transforms(image=img, mask=mask)\n                img  = data['image']\n                mask  = data['mask']\n                \n            return img, mask\n#             img = np.transpose(img, (2, 0, 1))\n#             msk = np.transpose(mask, (2, 0, 1))\n#             return torch.tensor(img), torch.tensor(mask)\n        else:\n            if self.transforms:\n                data = self.transforms(image=img)\n                img  = data['image']\n#             img = np.transpose(img, (2, 0, 1))\n#             return torch.tensor(img), id_, h, w\n            return img, id_, h, w\n        \n    @staticmethod\n    def load_img(path, size=CFG.IMG_SIZE):\n        img = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n        shape0 = np.array(img.shape[:2])\n        resize = np.array(size)\n        if np.any(shape0!=resize):\n            diff = resize - shape0\n            pad0 = diff[0]\n            pad1 = diff[1]\n            pady = [pad0//2, pad0//2 + pad0%2]\n            padx = [pad1//2, pad1//2 + pad1%2]\n            img = np.pad(img, [pady, padx])\n            img = img.reshape((*resize))\n        return img, shape0\n\n    @staticmethod\n    def load_imgs(img_paths, size=CFG.IMG_SIZE):\n        imgs = np.zeros((*size, len(img_paths)), dtype=np.float32)\n        for i, img_path in enumerate(img_paths):\n            if i==0:\n                img, shape0 = Dataset.load_img(img_path, size=size)\n            else:\n                img, _ = Dataset.load_img(img_path, size=size)\n            img = img.astype('float32') # original is uint16\n            mx = np.max(img)\n            if mx:\n                img/=mx # scale image to [0, 1]\n            imgs[..., i]+=img\n        return imgs, shape0\n\n    @staticmethod\n    def load_mask(path, size=CFG.IMG_SIZE):\n        msk = np.load(path)\n        shape0 = np.array(msk.shape[:2])\n        resize = np.array(size)\n        if np.any(shape0!=resize):\n            diff = resize - shape0\n            pad0 = diff[0]\n            pad1 = diff[1]\n            pady = [pad0//2, pad0//2 + pad0%2]\n            padx = [pad1//2, pad1//2 + pad1%2]\n            msk = np.pad(msk, [pady, padx, [0,0]])\n            msk = msk.reshape((*resize, 3))\n        msk = msk.astype('float32')\n        msk/=255.0\n        return msk","metadata":{"execution":{"iopub.status.busy":"2022-05-15T15:27:05.321461Z","iopub.execute_input":"2022-05-15T15:27:05.321785Z","iopub.status.idle":"2022-05-15T15:27:05.340142Z","shell.execute_reply.started":"2022-05-15T15:27:05.321752Z","shell.execute_reply":"2022-05-15T15:27:05.339479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SegmentationModel(nn.Module):\n    def __init__(self, arch:str ,encoder_name:str, encoder_weights:str, num_classes:int):\n        super(SegmentationModel, self).__init__()\n        self.arch = arch\n        self.encoder_name = encoder_name\n        self.encoder_weights = encoder_weights\n\n        self.model = smp.create_model(\n            self.arch,\n            encoder_name=self.encoder_name,\n            encoder_weights=self.encoder_weights,\n            in_channels=3,\n            classes=num_classes,\n            activation=None,\n        )\n            \n    def forward(self, images):\n        return self.model(images)\n    \ndef load_model(model, path):\n    model.load_state_dict(torch.load(path))\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-05-15T15:27:06.296301Z","iopub.execute_input":"2022-05-15T15:27:06.296862Z","iopub.status.idle":"2022-05-15T15:27:06.303945Z","shell.execute_reply.started":"2022-05-15T15:27:06.296821Z","shell.execute_reply":"2022-05-15T15:27:06.303219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cupy as cp\n\ndef mask2rle(msk, thr):\n    '''\n    img: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    '''\n    msk    = cp.array(msk)\n    pixels = msk.flatten()\n    pad    = cp.array([0])\n    pixels = cp.concatenate([pad, pixels, pad])\n    runs   = cp.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)\n\ndef masks2rles(msks, ids, heights, widths):\n    pred_strings = []; pred_ids = []; pred_classes = [];\n    for idx in range(msks.shape[0]):\n        msk = msks[idx]\n        height = heights[idx].item()\n        width = widths[idx].item()\n        shape0 = np.array([height, width])\n        resize = np.array([320, 384])\n        if np.any(shape0!=resize):\n            diff = resize - shape0\n            pad0 = diff[0]\n            pad1 = diff[1]\n            pady = [pad0//2, pad0//2 + pad0%2]\n            padx = [pad1//2, pad1//2 + pad1%2]\n            msk = msk[pady[0]:-pady[1], padx[0]:-padx[1], :]\n            msk = msk.reshape((*shape0, 3))\n        rle = [None]*3\n        for midx in [0, 1, 2]:\n            rle[midx] = mask2rle(msk[...,midx], CFG.THR)\n        pred_strings.extend(rle)\n        pred_ids.extend([ids[idx]]*len(rle))\n        pred_classes.extend(['large_bowel', 'small_bowel', 'stomach'])\n    return pred_strings, pred_ids, pred_classes","metadata":{"execution":{"iopub.status.busy":"2022-05-15T15:27:08.43883Z","iopub.execute_input":"2022-05-15T15:27:08.439454Z","iopub.status.idle":"2022-05-15T15:27:08.451402Z","shell.execute_reply.started":"2022-05-15T15:27:08.439401Z","shell.execute_reply":"2022-05-15T15:27:08.450427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@torch.no_grad()\ndef infer(test_loader, num_log=1, thr=CFG.THR):\n    msks = []; imgs = [];\n    pred_strings = []; pred_ids = []; pred_classes = [];\n    models = []\n    for fold in CFG.FOLDS:\n        model = SegmentationModel(CFG.ARCH, CFG.ENCODER_NAME, None, CFG.NUM_CLASSES)\n        model = load_model(model, CFG.saved_weights_dir / f'best_epoch-{fold:02d}.pt')\n        model.to(CFG.device)\n        model.eval()\n        models.append(model)\n    for idx, (img, ids, heights, widths) in enumerate(tqdm(test_loader, total=len(test_loader), desc='Infer ')):\n        img = img.to(CFG.device, dtype=torch.float) # .squeeze(0)\n        size = img.size()\n        msk = []\n        msk = torch.zeros((size[0], 3, size[2], size[3]), device=CFG.device, dtype=torch.float32)\n        for model in models:\n            out   = model(img) # .squeeze(0) # removing batch axis\n            out   = nn.Sigmoid()(out) # removing channel axis\n            msk+=out/len(models)\n        msk = (msk.permute((0,2,3,1))>thr).to(torch.uint8).cpu().detach().numpy() # shape: (n, h, w, c)\n        result = masks2rles(msk, ids, heights, widths)\n        pred_strings.extend(result[0])\n        pred_ids.extend(result[1])\n        pred_classes.extend(result[2])\n        if idx<num_log and CFG.DEBUG:\n            img = img.permute((0,2,3,1)).cpu().detach().numpy()\n            imgs.append(img[::5])\n            msks.append(msk[::5])\n        del img, msk, out, model, result\n        gc.collect()\n        torch.cuda.empty_cache()\n    return pred_strings, pred_ids, pred_classes, imgs, msks","metadata":{"execution":{"iopub.status.busy":"2022-05-15T15:45:56.802406Z","iopub.execute_input":"2022-05-15T15:45:56.802952Z","iopub.status.idle":"2022-05-15T15:45:56.817668Z","shell.execute_reply.started":"2022-05-15T15:45:56.802914Z","shell.execute_reply":"2022-05-15T15:45:56.816935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df = pd.read_csv(CFG.DATA_DIR / 'sample_submission.csv')\nif not len(sub_df):\n    CFG.DEBUG = True\n    sub_df = pd.read_csv(CFG.DATA_DIR / 'train.csv')\n    sub_df = sub_df[~sub_df.segmentation.isna()][:1000*3]\n    sub_df = sub_df.drop(columns=['class','segmentation']).drop_duplicates()\nelse:\n    CFG.DEBUG = False\n    sub_df = sub_df.drop(columns=['class','predicted']).drop_duplicates()\nsub_df = sub_df.progress_apply(get_metadata,axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-05-15T15:47:03.341462Z","iopub.execute_input":"2022-05-15T15:47:03.341806Z","iopub.status.idle":"2022-05-15T15:47:05.66283Z","shell.execute_reply.started":"2022-05-15T15:47:03.341766Z","shell.execute_reply":"2022-05-15T15:47:05.661979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if CFG.DEBUG:\n    paths = glob(f'/kaggle/input/uw-madison-gi-tract-image-segmentation/train/**/*png',recursive=True)\n#     paths = sorted(paths)\nelse:\n    paths = glob(f'/kaggle/input/uw-madison-gi-tract-image-segmentation/test/**/*png',recursive=True)\n#     paths = sorted(paths)\npath_df = pd.DataFrame(paths, columns=['image_path'])\npath_df = path_df.progress_apply(path2info, axis=1)\npath_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-15T15:47:05.6645Z","iopub.execute_input":"2022-05-15T15:47:05.664822Z","iopub.status.idle":"2022-05-15T15:48:34.035495Z","shell.execute_reply.started":"2022-05-15T15:47:05.664783Z","shell.execute_reply":"2022-05-15T15:48:34.034662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = sub_df.merge(path_df, on=['case','day','slice'], how='left')\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-15T15:48:34.037036Z","iopub.execute_input":"2022-05-15T15:48:34.037558Z","iopub.status.idle":"2022-05-15T15:48:34.060886Z","shell.execute_reply.started":"2022-05-15T15:48:34.037517Z","shell.execute_reply":"2022-05-15T15:48:34.060137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"channels=3\nstride=2\nfor i in range(channels):\n    test_df[f'image_path_{i:02}'] = test_df.groupby(['case','day'])['image_path'].shift(-i*stride).fillna(method=\"ffill\")\ntest_df['image_paths'] = test_df[[f'image_path_{i:02d}' for i in range(channels)]].values.tolist()\nif CFG.DEBUG:\n    test_df = test_df.sample(frac=1.0)\ntest_df.image_paths[0]","metadata":{"execution":{"iopub.status.busy":"2022-05-15T15:48:34.062782Z","iopub.execute_input":"2022-05-15T15:48:34.063065Z","iopub.status.idle":"2022-05-15T15:48:34.083756Z","shell.execute_reply.started":"2022-05-15T15:48:34.063027Z","shell.execute_reply":"2022-05-15T15:48:34.083071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = Dataset(test_df, transforms=get_transforms('valid'))\ntest_loader  = DataLoader(test_dataset, batch_size=CFG.VALID_BS, \n                          num_workers=2, shuffle=False, pin_memory=False)\npred_strings, pred_ids, pred_classes, imgs, msks = infer(test_loader)","metadata":{"execution":{"iopub.status.busy":"2022-05-15T15:48:34.085046Z","iopub.execute_input":"2022-05-15T15:48:34.085305Z","iopub.status.idle":"2022-05-15T15:49:06.84604Z","shell.execute_reply.started":"2022-05-15T15:48:34.085269Z","shell.execute_reply":"2022-05-15T15:49:06.845191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if CFG.DEBUG:\n    for img, msk in zip(imgs[0][:5], msks[0][:5]):\n        plt.figure(figsize=(12, 7))\n        plt.subplot(1, 3, 1); plt.imshow(img, cmap='bone');\n        plt.axis('OFF'); plt.title('image')\n        plt.subplot(1, 3, 2); plt.imshow(msk*255); plt.axis('OFF'); plt.title('mask')\n        plt.subplot(1, 3, 3); plt.imshow(img, cmap='bone'); plt.imshow(msk*255, alpha=0.4);\n        plt.axis('OFF'); plt.title('overlay')\n        plt.tight_layout()\n        plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-15T15:49:06.848048Z","iopub.execute_input":"2022-05-15T15:49:06.848472Z","iopub.status.idle":"2022-05-15T15:49:08.395882Z","shell.execute_reply.started":"2022-05-15T15:49:06.848419Z","shell.execute_reply":"2022-05-15T15:49:08.395199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del imgs, msks\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-05-15T15:49:08.39698Z","iopub.execute_input":"2022-05-15T15:49:08.397363Z","iopub.status.idle":"2022-05-15T15:49:08.633963Z","shell.execute_reply.started":"2022-05-15T15:49:08.397325Z","shell.execute_reply":"2022-05-15T15:49:08.633019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_df = pd.DataFrame({\n    \"id\":pred_ids,\n    \"class\":pred_classes,\n    \"predicted\":pred_strings\n})\nif not CFG.DEBUG:\n    sub_df = pd.read_csv(CFG.DATA_DIR / 'sample_submission.csv')\n    del sub_df['predicted']\nelse:\n    sub_df = pd.read_csv(CFG.DATA_DIR / 'train.csv')[:1000*3]\n    del sub_df['segmentation']\n    \nsub_df = sub_df.merge(pred_df, on=['id','class'])\nsub_df.to_csv('submission.csv',index=False)\ndisplay(sub_df.head(5))","metadata":{"execution":{"iopub.status.busy":"2022-05-15T15:49:08.635169Z","iopub.execute_input":"2022-05-15T15:49:08.635436Z","iopub.status.idle":"2022-05-15T15:49:08.932044Z","shell.execute_reply.started":"2022-05-15T15:49:08.635396Z","shell.execute_reply":"2022-05-15T15:49:08.931311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}